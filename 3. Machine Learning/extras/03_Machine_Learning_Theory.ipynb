{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Theory\n",
    "\n",
    "In this section we shall discuss the difference between classification and regression. \n",
    "\n",
    "- [Supervised learning](#supervised)\n",
    "- [What is classification?](#classification)\n",
    "- [What is regression?](#regression)\n",
    "    - [<mark> Exercise: Classification or Regression </mark>](#c-or-r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning Theory\n",
    "\n",
    "Afterwards we shall also build some intuition for some of the more common machine learning models. \n",
    "\n",
    "- [A brief look into machine learning models](#models)\n",
    "    - Decision Tree\n",
    "    - Random Forest\n",
    "    - Gradient Boosting\n",
    "    - K-Nearest Neighbour\n",
    "    - Support vector machine\n",
    "    - Linear Regression\n",
    "    - Polynomial Regression.\n",
    "    - Logistic Regression\n",
    "    - Neural Networks/Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning Theory\n",
    "\n",
    "Finally, we shall examine how to check if a model generalises well, by testing for under/over-fitting.\n",
    "\n",
    "- [Under/over-fitting](#fitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = 'supervised learning'></a>\n",
    "## Supervised Learning\n",
    "---\n",
    "The goal of supervised learning is to learn a function that can map a feature matrix (X) to a target vector (y).\n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/supervised_predictions.png' width=800px>\n",
    "\n",
    "The most common forms of supervised learning are **classification** and **regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "---\n",
    "<a id = 'classification'></a>\n",
    "<img src='../images/03_Machine_Learning_Theory/classification.jpeg' width=800px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='../images/03_Machine_Learning_Theory/robot-class.png' width=400px align='right'>\n",
    "\n",
    "## What is classification?\n",
    "\n",
    "When performing classification we try to identify the class or category that an observation belongs to.\n",
    "\n",
    "The target vector therefore consists of discrete values (or labels).\n",
    "\n",
    "Can you think of any examples of a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of Classification Problems\n",
    "<img src=../images/03_Machine_Learning_Theory/types-class.png width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Regression vs. Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you think of a situation where a classification algorithm wouldn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = 'regression'></a>\n",
    "## Regression \n",
    "---\n",
    "We've seen problems that require us to classify observations (e.g. type of penguin).\n",
    "\n",
    "We perform regression when our target variable is continuous. \n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/supervised_predictions.png' width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of Regression Problems\n",
    "<img src='../images/03_Machine_Learning_Theory/type-regress.png' width=700px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = 'c-or-r'></a>\n",
    "## <mark> Exercise: Classification or regression? </mark>\n",
    "\n",
    "1. Predict the height of a potted plant from the amount of rainfall.\n",
    "2. Identify whether a movie review is positive or negative.\n",
    "3. Decide whether the object in an image is a cat, dog or mouse.\n",
    "4. Predict the price of a house based upon information about its floor space and the area it is situated in.\n",
    "5. Predict whether a user is in their 20s, 30s, 40s, 50s or 60+ (what could be an issue with this framing of the problem?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = 'models'></a>\n",
    "## Machine Learning models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Tree\n",
    "\n",
    "This is a non-parametric algorithm that is able to perform both classification and regression by making a series of decisions based on the different features of the dataset. This series of decisions has a tree-like structure, which allows us to produce a readable decision diagram.\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/Decision-Tree-Algorithms.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 200px\"/>\n",
    "\n",
    "Each decision splits the tree into new branches. At the very end of the branches are leaf nodes that represent a class or regression outcome from the tree (unless tree depth is restricted). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensemble methods\n",
    "\n",
    "Ensemble models make predictions based on a number of different models. \n",
    "\n",
    "By combining individual models, the ensemble model tends to be more flexible (less bias) and less specific to the data it was trained on (less variance).\n",
    "\n",
    "Two most popular ensemble methods are bagging and boosting:\n",
    "- **Bagging**: Training a bunch of individual models in a *parallel* way. Each model is trained by a random subset of the data\n",
    "- **Boosting**: Training a bunch of individual models in a *sequential* way. Each individual model learns from mistakes made by the previous model.\n",
    "\n",
    "<!-- https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forest (bagging)\n",
    "\n",
    "A Random Forest ensembles a large number of independent unrestricted decision trees. The trees are trained on (bootstrapped) subsamples of the training data, with a random selection of features.\n",
    "\n",
    "After each tree produces a prediction, voting or averaging is used to produce the final prediction.\n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/randomforest.jpeg' width=400px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that sequentially trains models to address the mistakes, or residual error, of the models that came before.\n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/gradient-boosting.png' width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-Nearest Neighbour\n",
    "\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. For a new data point the model will look at the K-Nearest points and use that to classify the new data point.\n",
    "\n",
    "<font color='blue'>$\\text{Birds of a feather flock together}$</font>\n",
    "<img src=\"../images/03_Machine_Learning_Theory/KnnClassification.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "SVMs are powerful classifiers that are used for classifying a binary dataset into two classes with the help of hyperplanes.\n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/svm.gif' width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "<img src='../images/03_Machine_Learning_Theory/svm.png' width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "A linear approach to modeling the relationship between a dependent variable and one or more explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the case where there is one explanatory variable, the linear relationship is defined by a slope ($\\mathbf{m}$) and an intercept parameter ($\\mathbf{c}$):\n",
    "\n",
    "$$ y = \\mathbf{m} x + \\mathbf{c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, trying to predict the penguins body mass ($y$), from their bill depth ($x$):\n",
    "\n",
    "$$ y = Œ≤_0 + Œ≤_1x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "penguins = pd.read_csv('../data/penguins.csv')\n",
    "def simple_regression():\n",
    "    sns.regplot(x=penguins['bill_length_mm'], y=penguins['body_mass_g'],\n",
    "            scatter_kws={'s': 10, 'alpha': 0.2}, \n",
    "            line_kws={\"color\": \"red\", 'linewidth':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "simple_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Residuals \n",
    "\n",
    "We can examine how good the regression is by calculating the **residuals**: the distance from the line to the datapoints.\n",
    "<!-- \n",
    "![](https://cdn.kastatic.org/googleusercontent/Ebu4-AAwd4Z3irAQ9-AVyvA2abB-rb8cvQBjy60N42qD7JcDyd81bvz8DRiX6y2op9w2ryROslzP9OFtJ5PO9i6s) -->\n",
    "\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/residuals.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost function\n",
    "\n",
    "We aim to find the model that minimises a cost/objective function.\n",
    "\n",
    "We could consider the average residual value:\n",
    "\n",
    "$$\\Sigma_i^n \\frac{1}{n} y_i ‚Äì ≈∑_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What would be the issue with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost function\n",
    "\n",
    "To ensure residuals do not cancel each other out, it is better to use a method that is not dependent on the sign ($\\pm$) of the residual.\n",
    "\n",
    "For example, the Mean Absolute Error (MAE):\n",
    "\n",
    "$$\\Sigma_i^n \\frac{1}{n} |y_i ‚Äì ≈∑_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost function\n",
    "\n",
    "The most common choice is the Mean Squared Error (MSE):\n",
    "\n",
    "$$\\Sigma_i^n \\frac{1}{n}  (y_i ‚Äì ≈∑_i)^2$$\n",
    "\n",
    "This approach gives more weight to large residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "We may get a better model if we use information from more than one explanatory variable (**multiple** linear regression).\n",
    "\n",
    "For example, trying to predict the penguins body mass ($y$), from their bill length ($x_1$) and bill width ($x_2$):\n",
    "\n",
    "$$y=Œ≤_0 + Œ≤_1x_1 + Œ≤_2x_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Polynomial regression\n",
    "\n",
    "It may be that the explanatory variables have a polynomial relationship with the dependent variable.\n",
    "\n",
    "*Think of Einstein's famous $E = m c^2$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Polynomial regression\n",
    "\n",
    "To examine this, we can create polynomial versions of our explanatory variables and check if it gives us a better fit.\n",
    "\n",
    "<!-- ![](https://static.javatpoint.com/tutorial/machine-learning/../images/machine-learning-polynomial-regression.png) -->\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/poly.png\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, if there are two explanatory variables $ùë•_1$ and $ùë•_2$, a regression with quadratic features would look like so:\n",
    "\n",
    "$$ y = Œ≤_0 + Œ≤_1 x_1 + Œ≤_2 x_2 + Œ≤_3 x_1^2 + Œ≤_4 x_1 x_2 + Œ≤_5 x_2^2 $$\n",
    "\n",
    "where we have additional $Œ≤_3$, $Œ≤_4$ and $Œ≤_5$  parameters associated with the polynomial features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "With logistic regression we aim to predict the probability that the output = 1\n",
    "\n",
    "For example, the probability the penguin's sex is male ($y=1$), given their bill depth ($x$).\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/linear.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, the predictions ($y$) made using linear regression have a potentially infinite range (-$\\infty$, $\\infty$)\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/linear.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With logistic regression, we use a function ($S$) to ensure the values are bounded between (0,1). This is what gives logistic regression its famous \"S\" shape.\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/logistic.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $S$ is the Sigmoid function:\n",
    "$$ S(\\textbf{x}) = \\frac{1}{1+e^\\textbf{-x}} = \\frac{e^\\textbf{x}}{1+e^\\textbf{x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Networks/Deep Learning\n",
    "Artificial Neural Networks are modeled after the human brain and they can learn features directly from the data. They form their own subset of Machine Learning called Deep Learning.\n",
    "\n",
    "<!-- ![half center](https://miro.medium.com/max/1386/1*ZX05x1xYgaVoa4Vn2kKS9g.png) -->\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/nn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id = 'fitting'></a>\n",
    "## Under/over-fitting\n",
    "---\n",
    "\n",
    "A model's performance falls on a spectrum between these two regimes:\n",
    "\n",
    "* The model is **underfitting**: the model cannot learn the problem.\n",
    "* The model is **overfitting**: the model doesn't generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is an example of regression: the model starts off too simple but then gets too specific to the dataset, suggesting it will not generalize.\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/fitting_regimes.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is an example of classification: the model starts off too simple but then gets too specific to the dataset, suggesting it will not generalize.\n",
    "    \n",
    "<!-- ![](https://www.oreilly.com/library/view/deep-learning/9781491924570/assets/dpln_0107.png) -->\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/classification_fitting.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we only evaluate our model performance on the data we trained on, we won't understand how our model generalizes.\n",
    "\n",
    "Therefore we create seperate **train** and (hold out) **test** sets.\n",
    "\n",
    "<!-- <img src=\"https://miro.medium.com/max/2272/1*-8_kogvwmL1H6ooN1A1tsQ.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/> -->\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/train-test.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Underfitting vs overfitting**\n",
    "\n",
    "If a model performs badly on the train set, we know it is **underfitting**.\n",
    "\n",
    "If a model performs well on the train set, but badly on the test set it is **overfitting**.\n",
    "\n",
    "\n",
    "<img src=\"../images/03_Machine_Learning_Theory/under-over.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we made the distinction between two types of **supervised** machine learning:\n",
    " - Classification: where we attempt to distinguish between two or more classes;\n",
    " - Regression: where we attempt to predict a numerical value.\n",
    " \n",
    "We also briefly discussed some of the more common modelling techniques. This list is by no means exhaustive and in the latter notebooks we will discuss the mechanics of these algorithms in more detail.\n",
    "\n",
    "Finally, we discussed how to test if a model is able to generalize and what we mean by under/over-fitting."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
